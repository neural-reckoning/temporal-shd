{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcb827b1",
   "metadata": {},
   "source": [
    "# SHD Dataset Processing: Creating Timing-Based Benchmarks\n",
    "\n",
    "This notebook provides a complete pipeline for processing the Spiking Heidelberg Digits (SHD) dataset to create specialized benchmarks for evaluating temporal information processing in spiking neural networks.\n",
    "\n",
    "## Research Motivation\n",
    "\n",
    "The SHD dataset has become a widely used benchmark in neuromorphic computing. However, our research reveals that while these datasets contain substantial temporal information, surprisingly accurate classification can be achieved using spike counts alone or following various perturbations of spike timing. This suggests that standard SHD may not be ideal for probing the extent to which spike-based models can exploit temporal information.\n",
    "\n",
    "## Generated Datasets\n",
    "\n",
    "This notebook creates three dataset variants:\n",
    "\n",
    "1. **`shd_whole.mat`**: Complete original dataset (training + test combined)\n",
    "   - Contains all original spike data\n",
    "   - Serves as the baseline for comparison\n",
    "\n",
    "2. **`shd_part.mat`**: Partial dataset with class balancing\n",
    "   - Preserves original spike patterns and timing\n",
    "   - Balanced classes for fair evaluation\n",
    "   - Suitable for standard temporal processing benchmarks\n",
    "\n",
    "3. **`shd_norm.mat`**: Normalized dataset eliminating spike count information\n",
    "   - **By construction, eliminates all spike count information**\n",
    "   - Only temporal information remains (though some temporal info may be removed)\n",
    "   - Specifically designed to evaluate timing-based computation capabilities\n",
    "   - Enables fair assessment of temporal vs. rate-based processing\n",
    "\n",
    "## Dataset Access\n",
    "\n",
    "We provide a public release of these processed datasets to facilitate research on timing-based computation in spiking neural networks, as described in our paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546708a1",
   "metadata": {},
   "source": [
    "## Download Original SHD Dataset\n",
    "\n",
    "First, we need to download the original Spiking Heidelberg Digits dataset from the official source. The dataset consists of:\n",
    "- **Training set**: `shd_train.h5.gz` \n",
    "- **Test set**: `shd_test.h5.gz`\n",
    "\n",
    "All files are downloaded with MD5 hash verification to ensure data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "684833e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "import hashlib\n",
    "\n",
    "from six.moves.urllib.error import HTTPError \n",
    "from six.moves.urllib.error import URLError\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "print(\"  All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf915652",
   "metadata": {},
   "source": [
    "### Dataset Download Utilities\n",
    "\n",
    "The following functions handle secure file downloading, hash validation, and decompression. These utilities ensure data integrity and provide robust download capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fb7d655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hash validation functions defined.\n"
     ]
    }
   ],
   "source": [
    "def _hash_file(fpath, algorithm='sha256', chunk_size=65535):\n",
    "    \"\"\"Calculate hash of a file.\"\"\"\n",
    "    if (algorithm == 'sha256') or (algorithm == 'auto' and len(fpath) == 64):\n",
    "        hasher = hashlib.sha256()\n",
    "    else:\n",
    "        hasher = hashlib.md5()\n",
    "\n",
    "    with open(fpath, 'rb') as fpath_file:\n",
    "        for chunk in iter(lambda: fpath_file.read(chunk_size), b''):\n",
    "            hasher.update(chunk)\n",
    "\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def validate_file(fpath, file_hash, algorithm='auto', chunk_size=65535):\n",
    "    \"\"\"Validate a file against its hash.\"\"\"\n",
    "    if (algorithm == 'sha256') or (algorithm == 'auto' and len(file_hash) == 64):\n",
    "        hasher = 'sha256'\n",
    "    else:\n",
    "        hasher = 'md5'\n",
    "\n",
    "    if str(_hash_file(fpath, hasher, chunk_size)) == str(file_hash):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "print(\"  Hash validation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c5fb25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File download function defined.\n"
     ]
    }
   ],
   "source": [
    "def get_file(fname,\n",
    "             origin,\n",
    "             md5_hash=None,\n",
    "             file_hash=None,\n",
    "             cache_subdir='datasets',\n",
    "             hash_algorithm='auto',\n",
    "             extract=False,\n",
    "             archive_format='auto',\n",
    "             cache_dir=None):\n",
    "    \"\"\"Download a file from a URL if it not already in the cache.\"\"\"\n",
    "    \n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(os.path.expanduser('~'), '.data-cache')\n",
    "    if md5_hash is not None and file_hash is None:\n",
    "        file_hash = md5_hash\n",
    "        hash_algorithm = 'md5'\n",
    "    datadir_base = os.path.expanduser(cache_dir)\n",
    "    if not os.access(datadir_base, os.W_OK):\n",
    "        datadir_base = os.path.join('/tmp', '.data-cache')\n",
    "    datadir = os.path.join(datadir_base, cache_subdir)\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    os.makedirs(datadir, exist_ok=True)\n",
    "\n",
    "    fpath = os.path.join(datadir, fname)\n",
    "\n",
    "    download = False\n",
    "    if os.path.exists(fpath):\n",
    "        # File found; verify integrity if a hash was provided.\n",
    "        if file_hash is not None:\n",
    "            if not validate_file(fpath, file_hash, algorithm=hash_algorithm):\n",
    "                print('A local file was found, but it seems to be '\n",
    "                      'incomplete or outdated because the ' + hash_algorithm +\n",
    "                      ' file hash does not match the original value of ' + file_hash +\n",
    "                      ' so we will re-download the data.')\n",
    "                download = True\n",
    "    else:\n",
    "        download = True\n",
    "\n",
    "    if download:\n",
    "        print('Downloading data from', origin)\n",
    "\n",
    "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
    "        try:\n",
    "            try:\n",
    "                urlretrieve(origin, fpath)\n",
    "            except HTTPError as e:\n",
    "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
    "            except URLError as e:\n",
    "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
    "        except (Exception, KeyboardInterrupt) as e:\n",
    "            if os.path.exists(fpath):\n",
    "                os.remove(fpath)\n",
    "            raise\n",
    "\n",
    "    return fpath\n",
    "\n",
    "print(\"  File download function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb8efc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Decompression function defined.\n"
     ]
    }
   ],
   "source": [
    "def get_and_gunzip(origin, filename, md5hash=None, cache_dir=None, cache_subdir=None):\n",
    "    \"\"\"Download and decompress a gzipped file.\"\"\"\n",
    "    gz_file_path = get_file(filename, origin, md5_hash=md5hash, cache_dir=cache_dir, cache_subdir=cache_subdir)\n",
    "    hdf5_file_path = gz_file_path[:-3]  # Remove .gz extension\n",
    "    \n",
    "    if not os.path.isfile(hdf5_file_path) or os.path.getctime(gz_file_path) > os.path.getctime(hdf5_file_path):\n",
    "        print(\"Decompressing %s\" % gz_file_path)\n",
    "        with gzip.open(gz_file_path, 'rb') as f_in, open(hdf5_file_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    \n",
    "    return hdf5_file_path\n",
    "\n",
    "print(\"  Decompression function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77497c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Main dataset download function defined.\n"
     ]
    }
   ],
   "source": [
    "def get_shd_dataset(cache_dir, cache_subdir):\n",
    "    \"\"\"Download the Spiking Heidelberg Digits dataset.\"\"\"\n",
    "    \n",
    "    # The remote directory with the data files\n",
    "    base_url = \"https://zenkelab.org/datasets\"\n",
    "\n",
    "    # Retrieve MD5 hashes from remote\n",
    "    print(\"Fetching MD5 checksums...\")\n",
    "    response = urllib.request.urlopen(\"%s/md5sums.txt\" % base_url)\n",
    "    data = response.read() \n",
    "    lines = data.decode('utf-8').split(\"\\n\")\n",
    "    file_hashes = {line.split()[1]: line.split()[0] for line in lines if len(line.split()) == 2}\n",
    "\n",
    "    # Download the Spiking Heidelberg Digits (SHD) dataset\n",
    "    files = [\"shd_train.h5.gz\", \"shd_test.h5.gz\"]\n",
    "    \n",
    "    downloaded_files = []\n",
    "    for fn in files:\n",
    "        print(f\"\\nProcessing {fn}...\")\n",
    "        origin = \"%s/%s\" % (base_url, fn)\n",
    "        hdf5_file_path = get_and_gunzip(origin, fn, md5hash=file_hashes[fn], \n",
    "                                      cache_dir=cache_dir, cache_subdir=cache_subdir)\n",
    "        print(\"Available at: %s\" % hdf5_file_path)\n",
    "        downloaded_files.append(hdf5_file_path)\n",
    "    \n",
    "    return downloaded_files\n",
    "\n",
    "print(\"  Main dataset download function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d32044",
   "metadata": {},
   "source": [
    "## Configure Storage Location\n",
    "\n",
    "Set up the directory structure where the original and processed datasets will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cf262e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory: ./data\n",
      "Cache subdirectory: hdspikes\n",
      "Full cache path: ./data/hdspikes\n",
      "  Directory created/verified: ./data/hdspikes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set up cache directories relative to current working directory\n",
    "cache_dir = \"./data\"\n",
    "cache_subdir = \"hdspikes\"\n",
    "\n",
    "# Create the full path for easier reference\n",
    "full_cache_path = os.path.join(cache_dir, cache_subdir)\n",
    "\n",
    "print(f\"Cache directory: {cache_dir}\")\n",
    "print(f\"Cache subdirectory: {cache_subdir}\")\n",
    "print(f\"Full cache path: {full_cache_path}\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(full_cache_path, exist_ok=True)\n",
    "print(f\"  Directory created/verified: {full_cache_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1faf9d",
   "metadata": {},
   "source": [
    "## Execute Dataset Download\n",
    "\n",
    "Now we'll download the original SHD dataset. This process will:\n",
    "1. Fetch MD5 checksums from the server for data integrity\n",
    "2. Download the compressed training and test files  \n",
    "3. Verify file integrity using MD5 hash validation\n",
    "4. Automatically decompress the files to HDF5 format\n",
    "\n",
    "**Note**: Initial download may take several minutes depending on your internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31b577a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SHD dataset download...\n",
      "==================================================\n",
      "Fetching MD5 checksums...\n",
      "\n",
      "Processing shd_train.h5.gz...\n",
      "Downloading data from https://zenkelab.org/datasets/shd_train.h5.gz\n",
      "\n",
      "Processing shd_train.h5.gz...\n",
      "Downloading data from https://zenkelab.org/datasets/shd_train.h5.gz\n",
      "Decompressing ./data/hdspikes/shd_train.h5.gz\n",
      "Decompressing ./data/hdspikes/shd_train.h5.gz\n",
      "Available at: ./data/hdspikes/shd_train.h5\n",
      "\n",
      "Processing shd_test.h5.gz...\n",
      "Downloading data from https://zenkelab.org/datasets/shd_test.h5.gz\n",
      "Available at: ./data/hdspikes/shd_train.h5\n",
      "\n",
      "Processing shd_test.h5.gz...\n",
      "Downloading data from https://zenkelab.org/datasets/shd_test.h5.gz\n",
      "Decompressing ./data/hdspikes/shd_test.h5.gz\n",
      "Decompressing ./data/hdspikes/shd_test.h5.gz\n",
      "Available at: ./data/hdspikes/shd_test.h5\n",
      "\n",
      "==================================================\n",
      "Dataset download completed successfully!\n",
      "Files downloaded to: ./data/hdspikes\n",
      "Available at: ./data/hdspikes/shd_test.h5\n",
      "\n",
      "==================================================\n",
      "Dataset download completed successfully!\n",
      "Files downloaded to: ./data/hdspikes\n"
     ]
    }
   ],
   "source": [
    "# Download the SHD dataset\n",
    "print(\"Starting SHD dataset download...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    downloaded_files = get_shd_dataset(cache_dir, cache_subdir)\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Dataset download completed successfully!\")\n",
    "    print(f\"Files downloaded to: {full_cache_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n Error during download: {str(e)}\")\n",
    "    print(\"Please check your internet connection and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f25e31",
   "metadata": {},
   "source": [
    "## Verify Original Dataset\n",
    "\n",
    "Let's verify that the original SHD files were downloaded correctly and examine their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e95bf021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " File Verification Report\n",
      "========================================\n",
      " shd_train.h5\n",
      "    Path: ./data/hdspikes/shd_train.h5\n",
      "    Size: 256.37 MB (268,826,085 bytes)\n",
      "\n",
      " shd_test.h5\n",
      "    Path: ./data/hdspikes/shd_test.h5\n",
      "    Size: 75.07 MB (78,719,235 bytes)\n",
      "\n",
      " All files in cache directory:\n",
      "----------------------------------------\n",
      "   shd_test.h5 (75.07 MB)\n",
      "   shd_test.h5.gz (36.37 MB)\n",
      "   shd_train.h5 (256.37 MB)\n",
      "   shd_train.h5.gz (124.78 MB)\n",
      "\n",
      " Original SHD dataset is ready for processing!\n"
     ]
    }
   ],
   "source": [
    "# Check for the expected files\n",
    "expected_files = [\"shd_train.h5\", \"shd_test.h5\"]\n",
    "\n",
    "print(\" File Verification Report\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for filename in expected_files:\n",
    "    filepath = os.path.join(full_cache_path, filename)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        # Get file size\n",
    "        file_size = os.path.getsize(filepath)\n",
    "        file_size_mb = file_size / (1024 * 1024)\n",
    "        \n",
    "        print(f\" {filename}\")\n",
    "        print(f\"    Path: {filepath}\")\n",
    "        print(f\"    Size: {file_size_mb:.2f} MB ({file_size:,} bytes)\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\" {filename} - NOT FOUND\")\n",
    "        print(f\"   Expected at: {filepath}\")\n",
    "        print()\n",
    "\n",
    "# List all files in the cache directory\n",
    "print(\" All files in cache directory:\")\n",
    "print(\"-\" * 40)\n",
    "try:\n",
    "    all_files = os.listdir(full_cache_path)\n",
    "    for file in sorted(all_files):\n",
    "        file_path = os.path.join(full_cache_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"   {file} ({size_mb:.2f} MB)\")\n",
    "        else:\n",
    "            print(f\"   {file} (directory)\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"   Directory not found: {full_cache_path}\")\n",
    "\n",
    "print(\"\\n Original SHD dataset is ready for processing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26dc79c",
   "metadata": {},
   "source": [
    "## Explore Dataset Structure\n",
    "\n",
    "Let's examine the structure and content of the downloaded HDF5 files to understand the data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fe20b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading dataset files for exploration...\n",
      " Training file loaded: ./data/hdspikes/shd_train.h5\n",
      "   Keys: ['extra', 'labels', 'spikes']\n",
      " Test file loaded: ./data/hdspikes/shd_test.h5\n",
      "   Keys: ['extra', 'labels', 'spikes']\n",
      "\n",
      " Dataset Statistics:\n",
      "   Training samples: 8156\n",
      "   Test samples: 2264\n",
      "   Total samples: 10420\n",
      "   Unique labels: 20\n",
      "   Label range: 0 - 19\n",
      "\n",
      " Files closed successfully\n",
      "\n",
      " Ready to proceed with dataset processing pipeline!\n"
     ]
    }
   ],
   "source": [
    "# Example: How to load the SHD dataset\n",
    "import h5py\n",
    "\n",
    "# Open the dataset files\n",
    "train_file_path = os.path.join(full_cache_path, 'shd_train.h5')\n",
    "test_file_path = os.path.join(full_cache_path, 'shd_test.h5')\n",
    "\n",
    "try:\n",
    "    print(\" Loading dataset files for exploration...\")\n",
    "    \n",
    "    # Open training file\n",
    "    train_file = h5py.File(train_file_path, 'r')\n",
    "    print(f\" Training file loaded: {train_file_path}\")\n",
    "    print(f\"   Keys: {list(train_file.keys())}\")\n",
    "    \n",
    "    # Open test file  \n",
    "    test_file = h5py.File(test_file_path, 'r')\n",
    "    print(f\" Test file loaded: {test_file_path}\")\n",
    "    print(f\"   Keys: {list(test_file.keys())}\")\n",
    "    \n",
    "    # Access the data (without loading into memory)\n",
    "    x_train = train_file['spikes']\n",
    "    y_train = train_file['labels']\n",
    "    x_test = test_file['spikes']\n",
    "    y_test = test_file['labels']\n",
    "    \n",
    "    print(f\"\\n Dataset Statistics:\")\n",
    "    print(f\"   Training samples: {len(y_train)}\")\n",
    "    print(f\"   Test samples: {len(y_test)}\")\n",
    "    print(f\"   Total samples: {len(y_train) + len(y_test)}\")\n",
    "    print(f\"   Unique labels: {len(set(y_train[:]))}\")\n",
    "    print(f\"   Label range: {min(y_train[:])} - {max(y_train[:])}\")\n",
    "    \n",
    "    # Remember to close the files when done\n",
    "    train_file.close()\n",
    "    test_file.close()\n",
    "    print(\"\\n Files closed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Error loading dataset: {str(e)}\")\n",
    "\n",
    "print(\"\\n Ready to proceed with dataset processing pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2480f1",
   "metadata": {},
   "source": [
    "## Step 1: Generate Combined Dataset (shd_whole.mat)\n",
    "\n",
    "We first convert the sparse HDF5 format to a dense MAT format and combine training and test data. This creates our baseline `shd_whole.mat` file containing all original spike information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "302e3b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional libraries for MAT file generation imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Additional imports for MAT file generation\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy import io\n",
    "\n",
    "print(\"Additional libraries for MAT file generation imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62a1bc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse data generator function defined.\n"
     ]
    }
   ],
   "source": [
    "def sparse_data_generator_from_hdf5_spikes(X, y, batch_size, nb_steps, nb_units, max_time, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generator function to convert sparse spike data from HDF5 to dense format.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: HDF5 group containing 'times' and 'units' datasets\n",
    "    - y: Labels array\n",
    "    - batch_size: Number of samples per batch\n",
    "    - nb_steps: Number of time steps\n",
    "    - nb_units: Number of units/neurons\n",
    "    - max_time: Maximum time duration\n",
    "    - shuffle: Whether to shuffle the data\n",
    "    \n",
    "    Yields:\n",
    "    - dense_batch: Dense spike tensor of shape (batch_size, nb_units, nb_steps)\n",
    "    - y_batch: Labels for the batch\n",
    "    \"\"\"\n",
    "    labels_ = np.array(y, dtype=np.int32)\n",
    "    number_of_batches = len(labels_) // batch_size\n",
    "    sample_index = np.arange(len(labels_))\n",
    "    firing_times = X['times']\n",
    "    units_fired = X['units']\n",
    "    time_bins = np.linspace(0, max_time, num=nb_steps)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    counter = 0\n",
    "    while counter < number_of_batches:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        dense_batch = np.zeros((batch_size, nb_units, nb_steps), dtype=np.uint8)\n",
    "        y_batch = []\n",
    "\n",
    "        for bc, idx in enumerate(batch_index):\n",
    "            times = np.digitize(firing_times[idx], time_bins)\n",
    "            units = units_fired[idx]\n",
    "            times[times >= nb_steps] = nb_steps - 1 \n",
    "            dense_batch[bc, units, times] = 1\n",
    "            y_batch.append(labels_[idx])\n",
    "\n",
    "        yield dense_batch, np.array(y_batch, dtype=np.uint8)\n",
    "        counter += 1\n",
    "\n",
    "print(\"Sparse data generator function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20a14bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection helper function defined.\n"
     ]
    }
   ],
   "source": [
    "def collect_all(X_h5, Y_h5, batch_size, nb_steps, nb_units, max_time):\n",
    "    \"\"\"\n",
    "    Collect all data from HDF5 format and convert to dense numpy arrays.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_h5: HDF5 group containing spike data\n",
    "    - Y_h5: Labels array\n",
    "    - batch_size: Batch size for processing\n",
    "    - nb_steps: Number of time steps\n",
    "    - nb_units: Number of units/neurons\n",
    "    - max_time: Maximum time duration\n",
    "    \n",
    "    Returns:\n",
    "    - X_all: Dense spike data array\n",
    "    - Y_all: Labels array\n",
    "    \"\"\"\n",
    "    X_all = []\n",
    "    Y_all = []\n",
    "    for x_batch, y_batch in sparse_data_generator_from_hdf5_spikes(\n",
    "            X_h5, Y_h5, batch_size, nb_steps, nb_units, max_time, shuffle=False):\n",
    "        X_all.append(x_batch)\n",
    "        Y_all.append(y_batch)\n",
    "    X_all = np.concatenate(X_all, axis=0)\n",
    "    Y_all = np.concatenate(Y_all, axis=0)\n",
    "    return X_all, Y_all\n",
    "\n",
    "print(\"Data collection helper function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f55377f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAT file generation function defined.\n"
     ]
    }
   ],
   "source": [
    "def generate_shd_whole_mat(cache_dir, cache_subdir, save_path=None, \n",
    "                          batch_size=256, nb_steps=100, nb_units=700, max_time=1.4):\n",
    "    \"\"\"\n",
    "    Generate a combined MAT file containing both training and test SHD data in dense format.\n",
    "    \n",
    "    Parameters:\n",
    "    - cache_dir: Directory where the dataset is cached\n",
    "    - cache_subdir: Subdirectory containing the dataset files\n",
    "    - save_path: Path where to save the MAT file (if None, saves in cache directory)\n",
    "    - batch_size: Batch size for processing (default: 256)\n",
    "    - nb_steps: Number of time steps (default: 100)\n",
    "    - nb_units: Number of units/neurons (default: 700)\n",
    "    - max_time: Maximum time duration in seconds (default: 1.4)\n",
    "    \n",
    "    Returns:\n",
    "    - save_path: Path where the file was saved\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting MAT file generation...\")\n",
    "    print(f\"Parameters: batch_size={batch_size}, nb_steps={nb_steps}, nb_units={nb_units}, max_time={max_time}\")\n",
    "    \n",
    "    # Construct file paths\n",
    "    base_path = os.path.join(cache_dir, cache_subdir)\n",
    "    train_path = os.path.join(base_path, \"shd_train.h5\")\n",
    "    test_path = os.path.join(base_path, \"shd_test.h5\")\n",
    "    \n",
    "    # Set default save path if not provided\n",
    "    if save_path is None:\n",
    "        save_path = os.path.join(base_path, \"shd_whole.mat\")\n",
    "    \n",
    "    print(f\"   Loading data from:\")\n",
    "    print(f\"   Training: {train_path}\")\n",
    "    print(f\"   Test: {test_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open HDF5 files\n",
    "        train_file = h5py.File(train_path, \"r\")\n",
    "        test_file = h5py.File(test_path, \"r\")\n",
    "        \n",
    "        # Extract data\n",
    "        x_train = train_file['spikes']\n",
    "        y_train = train_file['labels']\n",
    "        x_test = test_file['spikes']\n",
    "        y_test = test_file['labels']\n",
    "        \n",
    "        print(\"   Converting sparse data to dense format...\")\n",
    "        print(\"   Processing training data...\")\n",
    "        X_train_all, Y_train_all = collect_all(x_train, y_train, batch_size, nb_steps, nb_units, max_time)\n",
    "        \n",
    "        print(\"   Processing test data...\")\n",
    "        X_test_all, Y_test_all = collect_all(x_test, y_test, batch_size, nb_steps, nb_units, max_time)\n",
    "        \n",
    "        # Combine training and test data\n",
    "        print(\" Combining training and test data...\")\n",
    "        X_all = np.concatenate([X_train_all, X_test_all], axis=0)\n",
    "        Y_all = np.concatenate([Y_train_all, Y_test_all], axis=0)\n",
    "        \n",
    "        print(f\"   Final dataset shape:\")\n",
    "        print(f\"   X shape: {X_all.shape}\")\n",
    "        print(f\"   Y shape: {Y_all.shape}\")\n",
    "        print(f\"   Total samples: {len(Y_all)}\")\n",
    "        print(f\"   Unique labels: {len(np.unique(Y_all))}\")\n",
    "        \n",
    "        # Save as MAT file\n",
    "        print(f\" Saving to: {save_path}\")\n",
    "        io.savemat(save_path, {'X': X_all, 'Y': Y_all})\n",
    "        \n",
    "        # Close files\n",
    "        train_file.close()\n",
    "        test_file.close()\n",
    "        \n",
    "        print(\" MAT file generation completed successfully!\")\n",
    "        return save_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error during MAT file generation: {str(e)}\")\n",
    "        if 'train_file' in locals():\n",
    "            train_file.close()\n",
    "        if 'test_file' in locals():\n",
    "            test_file.close()\n",
    "        raise\n",
    "\n",
    "print(\"MAT file generation function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070601ee",
   "metadata": {},
   "source": [
    "### Execute Baseline Dataset Generation\n",
    "\n",
    "Let's create the combined `shd_whole.mat` file that serves as our baseline containing all original spike information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b88a440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generating baseline combined SHD dataset...\n",
      "============================================================\n",
      "Starting MAT file generation...\n",
      "Parameters: batch_size=256, nb_steps=100, nb_units=700, max_time=1.4\n",
      "   Loading data from:\n",
      "   Training: ./data/hdspikes/shd_train.h5\n",
      "   Test: ./data/hdspikes/shd_test.h5\n",
      "   Converting sparse data to dense format...\n",
      "   Processing training data...\n",
      "   Processing test data...\n",
      "   Processing test data...\n",
      " Combining training and test data...\n",
      " Combining training and test data...\n",
      "   Final dataset shape:\n",
      "   X shape: (9984, 700, 100)\n",
      "   Y shape: (9984,)\n",
      "   Total samples: 9984\n",
      "   Unique labels: 20\n",
      " Saving to: ./data/hdspikes/shd_whole.mat\n",
      "   Final dataset shape:\n",
      "   X shape: (9984, 700, 100)\n",
      "   Y shape: (9984,)\n",
      "   Total samples: 9984\n",
      "   Unique labels: 20\n",
      " Saving to: ./data/hdspikes/shd_whole.mat\n",
      " MAT file generation completed successfully!\n",
      "============================================================\n",
      " Baseline dataset generation completed!\n",
      " File saved at: ./data/hdspikes/shd_whole.mat\n",
      " File size: 666.51 MB\n",
      " MAT file generation completed successfully!\n",
      "============================================================\n",
      " Baseline dataset generation completed!\n",
      " File saved at: ./data/hdspikes/shd_whole.mat\n",
      " File size: 666.51 MB\n",
      "   MAT file verification:\n",
      "   X shape: (9984, 700, 100)\n",
      "   Y shape: (1, 9984)\n",
      "   Data type X: uint8\n",
      "   Data type Y: uint8\n",
      "   MAT file verification:\n",
      "   X shape: (9984, 700, 100)\n",
      "   Y shape: (1, 9984)\n",
      "   Data type X: uint8\n",
      "   Data type Y: uint8\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "batch_size = 256\n",
    "nb_steps = 100\n",
    "nb_units = 700\n",
    "max_time = 1.4\n",
    "\n",
    "# You can customize the save path if needed\n",
    "# custom_save_path = \"/path/to/your/desired/location/shd_whole.mat\"\n",
    "custom_save_path = None  # Will save in the cache directory by default\n",
    "\n",
    "print(\" Generating baseline combined SHD dataset...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Generate the MAT file\n",
    "    saved_path = generate_shd_whole_mat(\n",
    "        cache_dir=cache_dir,\n",
    "        cache_subdir=cache_subdir,\n",
    "        save_path=custom_save_path,\n",
    "        batch_size=batch_size,\n",
    "        nb_steps=nb_steps,\n",
    "        nb_units=nb_units,\n",
    "        max_time=max_time\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\" Baseline dataset generation completed!\")\n",
    "    print(f\" File saved at: {saved_path}\")\n",
    "    \n",
    "    # Verify the saved file\n",
    "    if os.path.exists(saved_path):\n",
    "        file_size_mb = os.path.getsize(saved_path) / (1024 * 1024)\n",
    "        print(f\" File size: {file_size_mb:.2f} MB\")\n",
    "        \n",
    "        # Try to load and verify the MAT file\n",
    "        try:\n",
    "            mat_data = io.loadmat(saved_path)\n",
    "            print(f\"   MAT file verification:\")\n",
    "            print(f\"   X shape: {mat_data['X'].shape}\")\n",
    "            print(f\"   Y shape: {mat_data['Y'].shape}\")\n",
    "            print(f\"   Data type X: {mat_data['X'].dtype}\")\n",
    "            print(f\"   Data type Y: {mat_data['Y'].dtype}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Could not verify MAT file contents: {str(e)}\")\n",
    "    else:\n",
    "        print(\" File was not created successfully\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\" Error during baseline dataset generation: {str(e)}\")\n",
    "    print(\"Please check the dataset files and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf09b084",
   "metadata": {},
   "source": [
    "### Customization Options\n",
    "\n",
    "You can customize the MAT file generation by modifying the parameters:\n",
    "\n",
    "```python\n",
    "# Example with custom parameters\n",
    "custom_saved_path = generate_shd_whole_mat(\n",
    "    cache_dir=\"./data\",          # Your cache directory\n",
    "    cache_subdir=\"hdspikes\",     # Subdirectory with HDF5 files\n",
    "    save_path=\"./my_shd_data.mat\",  # Custom save location\n",
    "    batch_size=128,              # Different batch size\n",
    "    nb_steps=200,                # More time steps\n",
    "    nb_units=700,                # Number of units\n",
    "    max_time=1.4                 # Maximum time duration\n",
    ")\n",
    "```\n",
    "\n",
    "**Parameters Explanation:**\n",
    "- `batch_size`: Processing batch size (affects memory usage)\n",
    "- `nb_steps`: Number of time bins for temporal discretization\n",
    "- `nb_units`: Number of input units/neurons\n",
    "- `max_time`: Maximum time duration in seconds\n",
    "- `save_path`: Custom path for the output MAT file\n",
    "\n",
    "The generated MAT file contains:\n",
    "- `X`: Dense spike data array of shape `(total_samples, nb_units, nb_steps)`\n",
    "- `Y`: Labels array of shape `(total_samples,)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f208f3",
   "metadata": {},
   "source": [
    "## Step 2: Create Specialized Timing-Based Benchmarks\n",
    "\n",
    "Now we implement the core contribution of our research: creating specialized datasets that enable fair evaluation of temporal vs. rate-based information processing in spiking neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "621f31d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-count processing function defined.\n"
     ]
    }
   ],
   "source": [
    "def do_min_count(X, Y):\n",
    "    \"\"\"\n",
    "    Apply min-count processing to ensure each neuron has the same minimum number of spikes across all samples.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Input spike data of shape (N, F, T)\n",
    "    - Y: Labels array\n",
    "    \n",
    "    Returns:\n",
    "    - X_min: Processed spike data with min-count applied\n",
    "    - Y: Original labels (unchanged)\n",
    "    \"\"\"\n",
    "    N, F, T = X.shape\n",
    "    count_all = X.sum(axis=2)  # Count spikes per neuron per sample\n",
    "    min_counts = count_all.min(axis=0)  # Min count for each neuron across all samples\n",
    "\n",
    "    X_min = np.zeros_like(X)\n",
    "    for f_idx in range(F):\n",
    "        N_f = min_counts[f_idx]\n",
    "        if N_f == 0:\n",
    "            continue\n",
    "        for i_idx in range(N):\n",
    "            spike_times = np.where(X[i_idx, f_idx, :] == 1)[0]\n",
    "            if len(spike_times) > N_f:\n",
    "                # Randomly select N_f spike times\n",
    "                chosen_times = np.random.choice(spike_times, size=N_f, replace=False)\n",
    "                X_min[i_idx, f_idx, chosen_times] = 1\n",
    "            else:\n",
    "                # Keep all existing spikes\n",
    "                X_min[i_idx, f_idx, spike_times] = 1\n",
    "    return X_min, Y\n",
    "\n",
    "print(\"Min-count processing function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37b727e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced min-count processing function defined.\n"
     ]
    }
   ],
   "source": [
    "def create_min_count_dataset_avoid_widespread(X, Y, neuron_threshold=2, max_frac_for_neuron=0.01, max_samples_to_remove=1000):\n",
    "    \"\"\"\n",
    "    Create a min-count dataset while avoiding widespread neuron issues.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Input spike data of shape (N, F, T)\n",
    "    - Y: Labels array\n",
    "    - neuron_threshold: Minimum spike count threshold per neuron\n",
    "    - max_frac_for_neuron: Maximum fraction of samples to remove for any neuron\n",
    "    - max_samples_to_remove: Maximum total samples to remove\n",
    "    \n",
    "    Returns:\n",
    "    - X_processed: Processed spike data\n",
    "    - Y_processed: Corresponding labels\n",
    "    \"\"\"\n",
    "    N, F, T = X.shape\n",
    "    counts = X.sum(axis=2)  # Count spikes per neuron per sample\n",
    "    min_counts_per_neuron = counts.min(axis=0)\n",
    "\n",
    "    # Find problematic neurons\n",
    "    bad_neurons = np.where(min_counts_per_neuron < neuron_threshold)[0]\n",
    "    print(f\"Found {len(bad_neurons)} neurons with min_count < {neuron_threshold}.\")\n",
    "\n",
    "    if len(bad_neurons) == 0:\n",
    "        return do_min_count(X, Y)\n",
    "\n",
    "    # Identify samples to potentially remove\n",
    "    samples_to_remove = set()\n",
    "    for f_idx in bad_neurons:\n",
    "        neuron_counts = counts[:, f_idx]\n",
    "        i_bad = np.where(neuron_counts < neuron_threshold)[0]\n",
    "        frac = len(i_bad) / N\n",
    "        if frac <= max_frac_for_neuron:\n",
    "            samples_to_remove.update(i_bad)\n",
    "\n",
    "    # Apply sample filtering if reasonable\n",
    "    if 0 < len(samples_to_remove) < max_samples_to_remove:\n",
    "        keep_idxs = np.setdiff1d(np.arange(N), list(samples_to_remove))\n",
    "        X_filtered = X[keep_idxs]\n",
    "        Y_filtered = Y[keep_idxs]\n",
    "        print(f\"Removing {len(samples_to_remove)} samples.\")\n",
    "    else:\n",
    "        X_filtered = X\n",
    "        Y_filtered = Y\n",
    "        print(\"NOT removing any samples.\")\n",
    "\n",
    "    return do_min_count(X_filtered, Y_filtered)\n",
    "\n",
    "print(\"Advanced min-count processing function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a4d0952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balancing function defined.\n"
     ]
    }
   ],
   "source": [
    "def balance_dataset(X, Y):\n",
    "    \"\"\"\n",
    "    Balance the dataset by equalizing the number of samples per class.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Input spike data of shape (N, F, T)\n",
    "    - Y: Labels array\n",
    "    \n",
    "    Returns:\n",
    "    - X_balanced: Balanced spike data\n",
    "    - Y_balanced: Balanced labels\n",
    "    \"\"\"\n",
    "    N, num_neurons, T = X.shape\n",
    "    Y_flat = Y.ravel()\n",
    "\n",
    "    print(f\"Input dataset: X.shape=({N},{num_neurons},{T}), Y.shape=({len(Y_flat)})\")\n",
    "\n",
    "    # Count samples per class\n",
    "    unique_labels = np.unique(Y_flat)\n",
    "    counts_per_class = {c: np.sum(Y_flat == c) for c in unique_labels}\n",
    "    min_count = min(counts_per_class.values())\n",
    "\n",
    "    print(\"--- Sample counts per class ---\")\n",
    "    for c in sorted(unique_labels):\n",
    "        print(f\"Class {c}: {counts_per_class[c]} samples\")\n",
    "    print(f\"Using min_count = {min_count}\")\n",
    "\n",
    "    # Balance classes\n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "    for c in sorted(unique_labels):\n",
    "        idxs_c = np.where(Y_flat == c)[0]\n",
    "        np.random.shuffle(idxs_c)\n",
    "        selected = idxs_c[:min_count]\n",
    "        X_list.append(X[selected])\n",
    "        Y_list.append(Y_flat[selected])\n",
    "\n",
    "    X_balanced = np.concatenate(X_list, axis=0)\n",
    "    Y_balanced = np.concatenate(Y_list, axis=0)\n",
    "\n",
    "    # Shuffle the balanced dataset\n",
    "    perm = np.random.permutation(len(Y_balanced))\n",
    "    X_balanced = X_balanced[perm]\n",
    "    Y_balanced = Y_balanced[perm]\n",
    "\n",
    "    print(f\"Final balanced shape: X={X_balanced.shape}, Y={Y_balanced.shape}\")\n",
    "    return X_balanced, Y_balanced\n",
    "\n",
    "print(\"Class balancing function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ae4d2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Specialized dataset processing function defined.\n"
     ]
    }
   ],
   "source": [
    "def process_shd_whole_to_datasets(whole_mat_path, output_dir=None, \n",
    "                                  neuron_threshold=2, max_frac_for_neuron=0.01, \n",
    "                                  max_samples_to_remove=2000):\n",
    "    \"\"\"\n",
    "    Process shd_whole.mat to generate shd_part.mat and shd_norm.mat files.\n",
    "    \n",
    "    Parameters:\n",
    "    - whole_mat_path: Path to the shd_whole.mat file\n",
    "    - output_dir: Output directory (if None, uses same directory as input)\n",
    "    - neuron_threshold: Minimum spike count threshold per neuron\n",
    "    - max_frac_for_neuron: Maximum fraction of samples to remove for any neuron\n",
    "    - max_samples_to_remove: Maximum total samples to remove\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Paths to generated files\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\" Processing shd_whole.mat to generate part and norm datasets...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Set output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(whole_mat_path)\n",
    "    \n",
    "    # Define output paths\n",
    "    part_path = os.path.join(output_dir, \"shd_part.mat\")\n",
    "    norm_path = os.path.join(output_dir, \"shd_norm.mat\")\n",
    "    \n",
    "    try:\n",
    "        # Load the whole dataset\n",
    "        print(f\" Loading data from: {whole_mat_path}\")\n",
    "        if not os.path.exists(whole_mat_path):\n",
    "            raise FileNotFoundError(f\"File not found: {whole_mat_path}\")\n",
    "            \n",
    "        data = io.loadmat(whole_mat_path)\n",
    "        X_all = data['X']\n",
    "        Y_all = data['Y'].ravel()\n",
    "        \n",
    "        print(f\"   Loaded dataset:\")\n",
    "        print(f\"   X shape: {X_all.shape}\")\n",
    "        print(f\"   Y shape: {Y_all.shape}\")\n",
    "        print(f\"   Unique labels: {len(np.unique(Y_all))}\")\n",
    "        \n",
    "        # Step 1: Apply min-count processing and remove problematic samples\n",
    "        print(f\"\\n  Step 1: Applying min-count processing...\")\n",
    "        X_min, Y_min = create_min_count_dataset_avoid_widespread(\n",
    "            X_all, Y_all,\n",
    "            neuron_threshold=neuron_threshold,\n",
    "            max_frac_for_neuron=max_frac_for_neuron,\n",
    "            max_samples_to_remove=max_samples_to_remove\n",
    "        )\n",
    "        \n",
    "        # Step 2: Remove neurons that have zero activity across all samples and time\n",
    "        print(f\"\\n Step 2: Removing inactive neurons...\")\n",
    "        sum_over_samples_time = X_min.sum(axis=(0, 2))\n",
    "        zero_mask = (sum_over_samples_time == 0)\n",
    "        non_zero_mask = ~zero_mask\n",
    "        \n",
    "        print(f\"   Found {zero_mask.sum()} inactive neurons out of {len(zero_mask)}\")\n",
    "        \n",
    "        # Create normalized dataset (with min-count + removed inactive neurons)\n",
    "        X_norm = X_min[:, non_zero_mask, :]\n",
    "        print(f\"   After removing inactive neurons: X_norm.shape = {X_norm.shape}\")\n",
    "        \n",
    "        # Create part dataset (original spikes but same active neurons and same removed samples as norm)\n",
    "        X_part_unbalanced = X_all[:, non_zero_mask, :]\n",
    "        \n",
    "        # Important: Apply the same sample filtering to part dataset as was applied to norm dataset\n",
    "        # We need to use the same samples that were kept for the norm dataset\n",
    "        if X_min.shape[0] != X_all.shape[0]:\n",
    "            # Some samples were removed during min-count processing\n",
    "            print(f\"   Applying same sample filtering to part dataset...\")\n",
    "            print(f\"   Original samples: {X_all.shape[0]}, After min-count filtering: {X_min.shape[0]}\")\n",
    "            print(f\"   Samples removed: {X_all.shape[0] - X_min.shape[0]}\")\n",
    "            \n",
    "            # We need to track which samples were removed during the min-count processing\n",
    "            # The best approach is to re-run the sample identification logic\n",
    "            N, F, T = X_all.shape\n",
    "            counts = X_all.sum(axis=2)\n",
    "            min_counts_per_neuron = counts.min(axis=0)\n",
    "            bad_neurons = np.where(min_counts_per_neuron < neuron_threshold)[0]\n",
    "            \n",
    "            samples_to_remove = set()\n",
    "            if len(bad_neurons) > 0:\n",
    "                for f_idx in bad_neurons:\n",
    "                    neuron_counts = counts[:, f_idx]\n",
    "                    i_bad = np.where(neuron_counts < neuron_threshold)[0]\n",
    "                    frac = len(i_bad) / N\n",
    "                    if frac <= max_frac_for_neuron:\n",
    "                        samples_to_remove.update(i_bad)\n",
    "            \n",
    "            if 0 < len(samples_to_remove) < max_samples_to_remove:\n",
    "                kept_indices = np.setdiff1d(np.arange(N), list(samples_to_remove))\n",
    "                X_part_filtered = X_part_unbalanced[kept_indices]\n",
    "                Y_part_filtered = Y_all[kept_indices]\n",
    "                print(f\"   Applied same sample filtering: removed {len(samples_to_remove)} samples\")\n",
    "                print(f\"   Part dataset after filtering: {X_part_filtered.shape}\")\n",
    "            else:\n",
    "                print(f\"   No samples were removed in min-count processing\")\n",
    "                X_part_filtered = X_part_unbalanced\n",
    "                Y_part_filtered = Y_all\n",
    "        else:\n",
    "            # No samples were removed\n",
    "            X_part_filtered = X_part_unbalanced\n",
    "            Y_part_filtered = Y_all\n",
    "        \n",
    "        # Step 3: Apply class balancing\n",
    "        print(f\"\\n Step 3: Applying class balancing...\")\n",
    "        \n",
    "        print(f\"   Balancing part dataset...\")\n",
    "        X_part, Y_part = balance_dataset(X_part_filtered, Y_part_filtered)\n",
    "        \n",
    "        print(f\"   Balancing norm dataset...\")\n",
    "        X_norm_balanced, Y_norm = balance_dataset(X_norm, Y_min)\n",
    "        \n",
    "        # Step 4: Save the processed datasets\n",
    "        print(f\"\\n Step 4: Saving specialized benchmark datasets...\")\n",
    "        \n",
    "        # Save part dataset\n",
    "        print(f\"   Saving part dataset to: {part_path}\")\n",
    "        io.savemat(part_path, {\"X\": X_part, \"Y\": Y_part})\n",
    "        part_size_mb = os.path.getsize(part_path) / (1024 * 1024)\n",
    "        print(f\"    shd_part.mat saved ({part_size_mb:.2f} MB)\")\n",
    "        \n",
    "        # Save norm dataset  \n",
    "        print(f\"   Saving norm dataset to: {norm_path}\")\n",
    "        io.savemat(norm_path, {\"X\": X_norm_balanced, \"Y\": Y_norm})\n",
    "        norm_size_mb = os.path.getsize(norm_path) / (1024 * 1024)\n",
    "        print(f\"    shd_norm.mat saved ({norm_size_mb:.2f} MB)\")\n",
    "        \n",
    "        # Summary\n",
    "        print(f\"\\n Processing Summary:\")\n",
    "        print(f\"   Original dataset: {X_all.shape}\")\n",
    "        print(f\"   After sample filtering: {X_part_filtered.shape}\")\n",
    "        print(f\"   Part dataset (balanced): {X_part.shape}\")\n",
    "        print(f\"   Norm dataset (balanced): {X_norm_balanced.shape}\")\n",
    "        print(f\"   Active neurons: {non_zero_mask.sum()}/{len(non_zero_mask)}\")\n",
    "        print(f\"   Samples removed: {X_all.shape[0] - X_part_filtered.shape[0]}\")\n",
    "        \n",
    "        # Verify that both datasets have the same number of samples after balancing\n",
    "        if X_part.shape[0] == X_norm_balanced.shape[0]:\n",
    "            print(f\"    Both datasets have same sample count after balancing: {X_part.shape[0]}\")\n",
    "        else:\n",
    "            print(f\"     Different sample counts - Part: {X_part.shape[0]}, Norm: {X_norm_balanced.shape[0]}\")\n",
    "        \n",
    "        return {\n",
    "            \"part_path\": part_path,\n",
    "            \"norm_path\": norm_path,\n",
    "            \"part_shape\": X_part.shape,\n",
    "            \"norm_shape\": X_norm_balanced.shape,\n",
    "            \"active_neurons\": non_zero_mask.sum()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error during processing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "print(\" Specialized dataset processing function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841cfc7e",
   "metadata": {},
   "source": [
    "### Execute Specialized Dataset Generation\n",
    "\n",
    "Now let's create the two specialized benchmark datasets from our baseline `shd_whole.mat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90ddec1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting specialized dataset generation...\n",
      "======================================================================\n",
      "   Input file: ./data/hdspikes/shd_whole.mat\n",
      "   Processing parameters:\n",
      "   neuron_threshold: 2\n",
      "   max_frac_for_neuron: 0.01\n",
      "   max_samples_to_remove: 2000\n",
      " Processing shd_whole.mat to generate part and norm datasets...\n",
      "======================================================================\n",
      " Loading data from: ./data/hdspikes/shd_whole.mat\n",
      "   Loaded dataset:\n",
      "   X shape: (9984, 700, 100)\n",
      "   Y shape: (9984,)\n",
      "   Unique labels: 20\n",
      "\n",
      "  Step 1: Applying min-count processing...\n",
      "   Loaded dataset:\n",
      "   X shape: (9984, 700, 100)\n",
      "   Y shape: (9984,)\n",
      "   Unique labels: 20\n",
      "\n",
      "  Step 1: Applying min-count processing...\n",
      "Found 691 neurons with min_count < 2.\n",
      "Found 691 neurons with min_count < 2.\n",
      "Removing 1037 samples.\n",
      "Removing 1037 samples.\n",
      "\n",
      " Step 2: Removing inactive neurons...\n",
      "\n",
      " Step 2: Removing inactive neurons...\n",
      "   Found 476 inactive neurons out of 700\n",
      "   Found 476 inactive neurons out of 700\n",
      "   After removing inactive neurons: X_norm.shape = (8947, 224, 100)\n",
      "   Applying same sample filtering to part dataset...\n",
      "   Original samples: 9984, After min-count filtering: 8947\n",
      "   Samples removed: 1037\n",
      "   After removing inactive neurons: X_norm.shape = (8947, 224, 100)\n",
      "   Applying same sample filtering to part dataset...\n",
      "   Original samples: 9984, After min-count filtering: 8947\n",
      "   Samples removed: 1037\n",
      "   Applied same sample filtering: removed 1037 samples\n",
      "   Part dataset after filtering: (8947, 224, 100)\n",
      "\n",
      " Step 3: Applying class balancing...\n",
      "   Balancing part dataset...\n",
      "Input dataset: X.shape=(8947,224,100), Y.shape=(8947)\n",
      "--- Sample counts per class ---\n",
      "Class 0: 474 samples\n",
      "Class 1: 474 samples\n",
      "Class 2: 273 samples\n",
      "Class 3: 366 samples\n",
      "Class 4: 462 samples\n",
      "Class 5: 453 samples\n",
      "Class 6: 372 samples\n",
      "Class 7: 446 samples\n",
      "Class 8: 385 samples\n",
      "Class 9: 482 samples\n",
      "Class 10: 484 samples\n",
      "Class 11: 506 samples\n",
      "Class 12: 504 samples\n",
      "Class 13: 502 samples\n",
      "Class 14: 452 samples\n",
      "Class 15: 443 samples\n",
      "Class 16: 491 samples\n",
      "Class 17: 458 samples\n",
      "Class 18: 405 samples\n",
      "Class 19: 515 samples\n",
      "Using min_count = 273\n",
      "Final balanced shape: X=(5460, 224, 100), Y=(5460,)\n",
      "   Balancing norm dataset...\n",
      "Input dataset: X.shape=(8947,224,100), Y.shape=(8947)\n",
      "--- Sample counts per class ---\n",
      "Class 0: 474 samples\n",
      "Class 1: 474 samples\n",
      "Class 2: 273 samples\n",
      "Class 3: 366 samples\n",
      "Class 4: 462 samples\n",
      "Class 5: 453 samples\n",
      "Class 6: 372 samples\n",
      "Class 7: 446 samples\n",
      "Class 8: 385 samples\n",
      "Class 9: 482 samples\n",
      "Class 10: 484 samples\n",
      "Class 11: 506 samples\n",
      "Class 12: 504 samples\n",
      "Class 13: 502 samples\n",
      "Class 14: 452 samples\n",
      "Class 15: 443 samples\n",
      "Class 16: 491 samples\n",
      "Class 17: 458 samples\n",
      "Class 18: 405 samples\n",
      "Class 19: 515 samples\n",
      "Using min_count = 273\n",
      "   Applied same sample filtering: removed 1037 samples\n",
      "   Part dataset after filtering: (8947, 224, 100)\n",
      "\n",
      " Step 3: Applying class balancing...\n",
      "   Balancing part dataset...\n",
      "Input dataset: X.shape=(8947,224,100), Y.shape=(8947)\n",
      "--- Sample counts per class ---\n",
      "Class 0: 474 samples\n",
      "Class 1: 474 samples\n",
      "Class 2: 273 samples\n",
      "Class 3: 366 samples\n",
      "Class 4: 462 samples\n",
      "Class 5: 453 samples\n",
      "Class 6: 372 samples\n",
      "Class 7: 446 samples\n",
      "Class 8: 385 samples\n",
      "Class 9: 482 samples\n",
      "Class 10: 484 samples\n",
      "Class 11: 506 samples\n",
      "Class 12: 504 samples\n",
      "Class 13: 502 samples\n",
      "Class 14: 452 samples\n",
      "Class 15: 443 samples\n",
      "Class 16: 491 samples\n",
      "Class 17: 458 samples\n",
      "Class 18: 405 samples\n",
      "Class 19: 515 samples\n",
      "Using min_count = 273\n",
      "Final balanced shape: X=(5460, 224, 100), Y=(5460,)\n",
      "   Balancing norm dataset...\n",
      "Input dataset: X.shape=(8947,224,100), Y.shape=(8947)\n",
      "--- Sample counts per class ---\n",
      "Class 0: 474 samples\n",
      "Class 1: 474 samples\n",
      "Class 2: 273 samples\n",
      "Class 3: 366 samples\n",
      "Class 4: 462 samples\n",
      "Class 5: 453 samples\n",
      "Class 6: 372 samples\n",
      "Class 7: 446 samples\n",
      "Class 8: 385 samples\n",
      "Class 9: 482 samples\n",
      "Class 10: 484 samples\n",
      "Class 11: 506 samples\n",
      "Class 12: 504 samples\n",
      "Class 13: 502 samples\n",
      "Class 14: 452 samples\n",
      "Class 15: 443 samples\n",
      "Class 16: 491 samples\n",
      "Class 17: 458 samples\n",
      "Class 18: 405 samples\n",
      "Class 19: 515 samples\n",
      "Using min_count = 273\n",
      "Final balanced shape: X=(5460, 224, 100), Y=(5460,)\n",
      "\n",
      " Step 4: Saving specialized benchmark datasets...\n",
      "   Saving part dataset to: ./data/hdspikes/shd_part.mat\n",
      "Final balanced shape: X=(5460, 224, 100), Y=(5460,)\n",
      "\n",
      " Step 4: Saving specialized benchmark datasets...\n",
      "   Saving part dataset to: ./data/hdspikes/shd_part.mat\n",
      "    shd_part.mat saved (116.64 MB)\n",
      "   Saving norm dataset to: ./data/hdspikes/shd_norm.mat\n",
      "    shd_part.mat saved (116.64 MB)\n",
      "   Saving norm dataset to: ./data/hdspikes/shd_norm.mat\n",
      "    shd_norm.mat saved (116.64 MB)\n",
      "\n",
      " Processing Summary:\n",
      "   Original dataset: (9984, 700, 100)\n",
      "   After sample filtering: (8947, 224, 100)\n",
      "   Part dataset (balanced): (5460, 224, 100)\n",
      "   Norm dataset (balanced): (5460, 224, 100)\n",
      "   Active neurons: 224/700\n",
      "   Samples removed: 1037\n",
      "    Both datasets have same sample count after balancing: 5460\n",
      "======================================================================\n",
      "  Specialized dataset generation completed!\n",
      "  Generated timing-based benchmark files:\n",
      "     shd_part.mat: ./data/hdspikes/shd_part.mat\n",
      "     shd_norm.mat: ./data/hdspikes/shd_norm.mat\n",
      "    Part dataset: 116.64 MB\n",
      "      X shape: (5460, 224, 100)\n",
      "      Y shape: (1, 5460)\n",
      "      Unique labels: 20\n",
      "    Norm dataset: 116.64 MB\n",
      "      X shape: (5460, 224, 100)\n",
      "      Y shape: (1, 5460)\n",
      "      Unique labels: 20\n",
      "\n",
      " Final Statistics:\n",
      "   Active neurons used: 224\n",
      "   Part dataset shape: (5460, 224, 100)\n",
      "   Norm dataset shape: (5460, 224, 100)\n",
      "    shd_norm.mat saved (116.64 MB)\n",
      "\n",
      " Processing Summary:\n",
      "   Original dataset: (9984, 700, 100)\n",
      "   After sample filtering: (8947, 224, 100)\n",
      "   Part dataset (balanced): (5460, 224, 100)\n",
      "   Norm dataset (balanced): (5460, 224, 100)\n",
      "   Active neurons: 224/700\n",
      "   Samples removed: 1037\n",
      "    Both datasets have same sample count after balancing: 5460\n",
      "======================================================================\n",
      "  Specialized dataset generation completed!\n",
      "  Generated timing-based benchmark files:\n",
      "     shd_part.mat: ./data/hdspikes/shd_part.mat\n",
      "     shd_norm.mat: ./data/hdspikes/shd_norm.mat\n",
      "    Part dataset: 116.64 MB\n",
      "      X shape: (5460, 224, 100)\n",
      "      Y shape: (1, 5460)\n",
      "      Unique labels: 20\n",
      "    Norm dataset: 116.64 MB\n",
      "      X shape: (5460, 224, 100)\n",
      "      Y shape: (1, 5460)\n",
      "      Unique labels: 20\n",
      "\n",
      " Final Statistics:\n",
      "   Active neurons used: 224\n",
      "   Part dataset shape: (5460, 224, 100)\n",
      "   Norm dataset shape: (5460, 224, 100)\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration parameters\n",
    "neuron_threshold = 2\n",
    "max_frac_for_neuron = 0.01\n",
    "max_samples_to_remove = 2000\n",
    "\n",
    "# Path to the shd_whole.mat file (generated in previous step)\n",
    "whole_mat_path = os.path.join(full_cache_path, \"shd_whole.mat\")\n",
    "\n",
    "print(\" Starting specialized dataset generation...\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   Input file: {whole_mat_path}\")\n",
    "print(f\"   Processing parameters:\")\n",
    "print(f\"   neuron_threshold: {neuron_threshold}\")\n",
    "print(f\"   max_frac_for_neuron: {max_frac_for_neuron}\")\n",
    "print(f\"   max_samples_to_remove: {max_samples_to_remove}\")\n",
    "\n",
    "try:\n",
    "    # Check if the whole mat file exists\n",
    "    if not os.path.exists(whole_mat_path):\n",
    "        print(f\" File not found: {whole_mat_path}\")\n",
    "        print(\"  Please run the baseline MAT file generation step first!\")\n",
    "    else:\n",
    "        # Process the dataset\n",
    "        results = process_shd_whole_to_datasets(\n",
    "            whole_mat_path=whole_mat_path,\n",
    "            output_dir=None,  # Will save in the same directory\n",
    "            neuron_threshold=neuron_threshold,\n",
    "            max_frac_for_neuron=max_frac_for_neuron,\n",
    "            max_samples_to_remove=max_samples_to_remove\n",
    "        )\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(\"  Specialized dataset generation completed!\")\n",
    "        print(f\"  Generated timing-based benchmark files:\")\n",
    "        print(f\"     shd_part.mat: {results['part_path']}\")\n",
    "        print(f\"     shd_norm.mat: {results['norm_path']}\")\n",
    "        \n",
    "        # Verify the generated files\n",
    "        for name, path in [(\"Part\", results['part_path']), (\"Norm\", results['norm_path'])]:\n",
    "            if os.path.exists(path):\n",
    "                file_size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "                print(f\"    {name} dataset: {file_size_mb:.2f} MB\")\n",
    "                \n",
    "                # Load and verify\n",
    "                try:\n",
    "                    verify_data = io.loadmat(path)\n",
    "                    print(f\"      X shape: {verify_data['X'].shape}\")\n",
    "                    print(f\"      Y shape: {verify_data['Y'].shape}\")\n",
    "                    print(f\"      Unique labels: {len(np.unique(verify_data['Y']))}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"        Could not verify file: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"     {name} dataset: File not created\")\n",
    "        \n",
    "        print(f\"\\n Final Statistics:\")\n",
    "        print(f\"   Active neurons used: {results['active_neurons']}\")\n",
    "        print(f\"   Part dataset shape: {results['part_shape']}\")\n",
    "        print(f\"   Norm dataset shape: {results['norm_shape']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\" Error during specialized dataset generation: {str(e)}\")\n",
    "    print(\"Please check the input file and parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d95f44",
   "metadata": {},
   "source": [
    "### Dataset Processing Summary\n",
    "\n",
    "The advanced processing pipeline generates three types of datasets:\n",
    "\n",
    "1. **`shd_whole.mat`**: Complete combined dataset (training + test)\n",
    "   - Contains all original spike data\n",
    "   - No preprocessing applied\n",
    "   - Used as the base for further processing\n",
    "\n",
    "2. **`shd_part.mat`**: Partial dataset with class balancing\n",
    "   - Original spike patterns preserved\n",
    "   - **Same sample filtering as norm dataset applied**\n",
    "   - Inactive neurons removed\n",
    "   - Classes balanced (equal samples per class)\n",
    "   - **Should have same sample count as norm dataset**\n",
    "   - Suitable for standard training\n",
    "\n",
    "3. **`shd_norm.mat`**: Normalized dataset with min-count processing\n",
    "   - Min-count spike normalization applied\n",
    "   - Inactive neurons removed\n",
    "   - Classes balanced\n",
    "   - Problematic samples filtered out\n",
    "   - Suitable for specialized training scenarios\n",
    "\n",
    "**Processing Steps:**\n",
    "1. **Min-count processing**: Ensures consistent spike counts across neurons (applies to norm only)\n",
    "2. **Sample filtering**: Removes problematic samples (applies to both datasets equally)\n",
    "3. **Neuron filtering**: Removes neurons with zero activity (applies to both datasets)\n",
    "4. **Class balancing**: Equalizes the number of samples per class (applies to both datasets)\n",
    "5. **Shuffling**: Randomizes sample order (applies to both datasets)\n",
    "\n",
    "**Key Improvement:**\n",
    "- Both `part` and `norm` datasets now use the same filtered samples before class balancing\n",
    "- This ensures consistent comparison between the two dataset variants\n",
    "- Both datasets will have identical sample counts after balancing (e.g., 5460 samples each)\n",
    "\n",
    "**Customization Options:**\n",
    "- `neuron_threshold`: Minimum spike count per neuron (default: 2)\n",
    "- `max_frac_for_neuron`: Maximum fraction of samples to remove (default: 0.01)  \n",
    "- `max_samples_to_remove`: Maximum total samples to remove (default: 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe4a1d",
   "metadata": {},
   "source": [
    "## Step 3: Verify Generated Timing-Based Benchmarks\n",
    "\n",
    "Let's load and verify our specialized timing-based benchmark datasets to ensure they meet our research requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dc3db4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading specialized timing-based benchmark datasets...\n",
      "======================================================================\n",
      " Part (spike pattern): 116.64 MB\n",
      "    Dataset shape: X=(5460, 224, 100), Y=(1, 5460)\n",
      "    Number of classes: 20\n",
      "    Total samples: 5460\n",
      "    Total spikes: 17,584,343\n",
      "    Average spikes per sample: 3220.58\n",
      "    Class distribution: {0: 273, 1: 273, 2: 273, 3: 273, 4: 273, 5: 273, 6: 273, 7: 273, 8: 273, 9: 273, 10: 273, 11: 273, 12: 273, 13: 273, 14: 273, 15: 273, 16: 273, 17: 273, 18: 273, 19: 273}\n",
      "\n",
      " Norm (normalized): 116.64 MB\n",
      "    Dataset shape: X=(5460, 224, 100), Y=(1, 5460)\n",
      "    Number of classes: 20\n",
      "    Total samples: 5460\n",
      "    Total spikes: 2,571,660\n",
      "    Average spikes per sample: 471.00\n",
      "    Class distribution: {0: 273, 1: 273, 2: 273, 3: 273, 4: 273, 5: 273, 6: 273, 7: 273, 8: 273, 9: 273, 10: 273, 11: 273, 12: 273, 13: 273, 14: 273, 15: 273, 16: 273, 17: 273, 18: 273, 19: 273}\n",
      "\n",
      "  Research Insight:\n",
      "   • Part dataset: Contains original spike patterns (temporal information)\n",
      "   • Norm dataset: Min-count normalized (removes rate information)\n",
      "   • Both datasets are class-balanced for fair comparison\n",
      "   • Same problematic samples removed from both variants\n"
     ]
    }
   ],
   "source": [
    "# Load and verify the specialized datasets\n",
    "data_dir = full_cache_path\n",
    "part_path = os.path.join(data_dir, 'shd_part.mat')\n",
    "norm_path = os.path.join(data_dir, 'shd_norm.mat')\n",
    "\n",
    "print(\" Loading specialized timing-based benchmark datasets...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Verify files exist\n",
    "files_to_check = [\n",
    "    (\"Part (spike pattern)\", part_path),\n",
    "    (\"Norm (normalized)\", norm_path)\n",
    "]\n",
    "\n",
    "for name, filepath in files_to_check:\n",
    "    if os.path.exists(filepath):\n",
    "        file_size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        print(f\" {name}: {file_size_mb:.2f} MB\")\n",
    "        \n",
    "        # Load the data\n",
    "        data = io.loadmat(filepath)\n",
    "        \n",
    "        print(f\"    Dataset shape: X={data['X'].shape}, Y={data['Y'].shape}\")\n",
    "        print(f\"    Number of classes: {len(np.unique(data['Y']))}\")\n",
    "        print(f\"    Total samples: {data['X'].shape[0]}\")\n",
    "        \n",
    "        # Check spike statistics for this dataset\n",
    "        total_spikes = np.sum(data['X'])\n",
    "        avg_spikes_per_sample = total_spikes / data['X'].shape[0]\n",
    "        print(f\"    Total spikes: {total_spikes:,}\")\n",
    "        print(f\"    Average spikes per sample: {avg_spikes_per_sample:.2f}\")\n",
    "        \n",
    "        # Check class distribution\n",
    "        unique_labels, counts = np.unique(data['Y'], return_counts=True)\n",
    "        print(f\"    Class distribution: {dict(zip(unique_labels.flatten(), counts))}\")\n",
    "        \n",
    "        print()\n",
    "    else:\n",
    "        print(f\" {name}: File not found at {filepath}\")\n",
    "\n",
    "print(\"  Research Insight:\")\n",
    "print(\"   • Part dataset: Contains original spike patterns (temporal information)\")\n",
    "print(\"   • Norm dataset: Min-count normalized (removes rate information)\")\n",
    "print(\"   • Both datasets are class-balanced for fair comparison\")\n",
    "print(\"   • Same problematic samples removed from both variants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c932ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Comparative Analysis of Timing-Based Benchmarks\n",
      "======================================================================\n",
      "   Dataset Shape Comparison:\n",
      "   Part dataset: (5460, 224, 100)\n",
      "   Norm dataset: (5460, 224, 100)\n",
      "    Both datasets have identical shapes\n",
      "\n",
      " Spike Count Analysis:\n",
      "   Part dataset total spikes: 17,584,343\n",
      "   Norm dataset total spikes: 2,571,660\n",
      "   Spike reduction ratio: 0.146\n",
      "\n",
      " Spike Distribution Per Sample:\n",
      "   Part dataset - Mean: 3220.58, Std: 897.56\n",
      "   Norm dataset - Mean: 471.00, Std: 0.00\n",
      "\n",
      " Research Implications:\n",
      "   • Part dataset preserves temporal spike patterns and rate information\n",
      "   • Norm dataset removes rate information while preserving timing patterns\n",
      "   • Performance differences indicate reliance on rate vs. temporal coding\n",
      "   • Both datasets enable fair comparison with identical preprocessing\n",
      "\n",
      " Spike Count Analysis:\n",
      "   Part dataset total spikes: 17,584,343\n",
      "   Norm dataset total spikes: 2,571,660\n",
      "   Spike reduction ratio: 0.146\n",
      "\n",
      " Spike Distribution Per Sample:\n",
      "   Part dataset - Mean: 3220.58, Std: 897.56\n",
      "   Norm dataset - Mean: 471.00, Std: 0.00\n",
      "\n",
      " Research Implications:\n",
      "   • Part dataset preserves temporal spike patterns and rate information\n",
      "   • Norm dataset removes rate information while preserving timing patterns\n",
      "   • Performance differences indicate reliance on rate vs. temporal coding\n",
      "   • Both datasets enable fair comparison with identical preprocessing\n"
     ]
    }
   ],
   "source": [
    "# Comparative analysis between the two benchmark datasets\n",
    "print(\"\\n Comparative Analysis of Timing-Based Benchmarks\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if os.path.exists(part_path) and os.path.exists(norm_path):\n",
    "    # Load both datasets\n",
    "    part_data = io.loadmat(part_path)\n",
    "    norm_data = io.loadmat(norm_path)\n",
    "    \n",
    "    X_part, Y_part = part_data['X'], part_data['Y']\n",
    "    X_norm, Y_norm = norm_data['X'], norm_data['Y']\n",
    "    \n",
    "    # Verify identical shapes and sample counts\n",
    "    print(f\"   Dataset Shape Comparison:\")\n",
    "    print(f\"   Part dataset: {X_part.shape}\")\n",
    "    print(f\"   Norm dataset: {X_norm.shape}\")\n",
    "    \n",
    "    if X_part.shape == X_norm.shape:\n",
    "        print(f\"    Both datasets have identical shapes\")\n",
    "    else:\n",
    "        print(f\"     Different shapes detected\")\n",
    "    \n",
    "    # Spike count comparison\n",
    "    part_total_spikes = np.sum(X_part)\n",
    "    norm_total_spikes = np.sum(X_norm)\n",
    "    \n",
    "    print(f\"\\n Spike Count Analysis:\")\n",
    "    print(f\"   Part dataset total spikes: {part_total_spikes:,}\")\n",
    "    print(f\"   Norm dataset total spikes: {norm_total_spikes:,}\")\n",
    "    print(f\"   Spike reduction ratio: {norm_total_spikes/part_total_spikes:.3f}\")\n",
    "    \n",
    "    # Per-sample spike distribution\n",
    "    part_spikes_per_sample = np.sum(X_part, axis=(1,2))\n",
    "    norm_spikes_per_sample = np.sum(X_norm, axis=(1,2))\n",
    "    \n",
    "    print(f\"\\n Spike Distribution Per Sample:\")\n",
    "    print(f\"   Part dataset - Mean: {np.mean(part_spikes_per_sample):.2f}, Std: {np.std(part_spikes_per_sample):.2f}\")\n",
    "    print(f\"   Norm dataset - Mean: {np.mean(norm_spikes_per_sample):.2f}, Std: {np.std(norm_spikes_per_sample):.2f}\")\n",
    "    \n",
    "    # Research implications\n",
    "    print(f\"\\n Research Implications:\")\n",
    "    print(f\"   • Part dataset preserves temporal spike patterns and rate information\")\n",
    "    print(f\"   • Norm dataset removes rate information while preserving timing patterns\")\n",
    "    print(f\"   • Performance differences indicate reliance on rate vs. temporal coding\")\n",
    "    print(f\"   • Both datasets enable fair comparison with identical preprocessing\")\n",
    "    \n",
    "else:\n",
    "    print(\"  Cannot perform comparative analysis - datasets not found\")\n",
    "    print(\"   Please ensure both datasets have been generated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac662c81",
   "metadata": {},
   "source": [
    "## Conclusion: Enabling Fair Assessment of Temporal vs. Rate-Based Processing\n",
    "\n",
    "This notebook provides a complete pipeline for generating specialized timing-based benchmarks from the SHD dataset, enabling researchers to conduct fair assessments of temporal versus rate-based information processing in spiking neural networks.\n",
    "\n",
    "### 🎯 Key Contributions\n",
    "\n",
    "1. **Baseline Dataset Generation (`shd_whole.mat`)**\n",
    "   - Combines training and test data into a unified format\n",
    "   - Preserves original temporal structure\n",
    "   - Provides foundation for specialized processing\n",
    "\n",
    "2. **Timing-Based Benchmark Datasets**\n",
    "   - **`shd_part.mat`**: Preserves both temporal patterns and rate information\n",
    "   - **`shd_norm.mat`**: Eliminates rate information via min-count normalization\n",
    "   - Both datasets undergo identical preprocessing for fair comparison\n",
    "\n",
    "3. **Reproducible Research Pipeline**\n",
    "   - Standardized processing parameters\n",
    "   - Consistent sample filtering across dataset variants\n",
    "   - Class balancing for unbiased evaluation\n",
    "   - Comprehensive validation and verification\n",
    "\n",
    "### 🔬 Research Applications\n",
    "\n",
    "- **Temporal Coding Research**: Compare model performance between `part` and `norm` datasets\n",
    "- **Rate vs. Timing Analysis**: Performance differences indicate information encoding preferences\n",
    "- **Neuromorphic Benchmarking**: Standardized datasets for fair model comparison\n",
    "- **Algorithm Development**: Controlled environment for testing temporal processing algorithms\n",
    "\n",
    "### 📈 Usage Recommendations\n",
    "\n",
    "1. **Baseline Evaluation**: Train models on `shd_part.mat` for standard performance assessment\n",
    "2. **Temporal Sensitivity Testing**: Compare performance between `part` and `norm` datasets\n",
    "3. **Algorithm Comparison**: Use identical preprocessing to ensure fair model comparisons\n",
    "4. **Research Publication**: Reference this pipeline for reproducible results\n",
    "\n",
    "### 🚀 Future Extensions\n",
    "\n",
    "- Support for additional neuromorphic datasets\n",
    "- Extended normalization techniques\n",
    "- Advanced temporal analysis metrics\n",
    "- Integration with popular neuromorphic frameworks\n",
    "\n",
    "---\n",
    "\n",
    "**Citation**: If you use these timing-based benchmarks in your research, please cite the associated publication and acknowledge this processing pipeline.\n",
    "\n",
    "**Reproducibility**: All processing steps are deterministic (with fixed random seed) to ensure reproducible results across different research groups."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
