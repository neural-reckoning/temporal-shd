{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2cc35fc",
   "metadata": {},
   "source": [
    "# SHD & SSC Dataset Loader from Zenodo\n",
    "\n",
    "Download neuromorphic datasets (SHD-Norm and SSC-Norm) from Zenodo and create data loaders for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f1628cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Downloading Neuromorphic Datasets from Zenodo ===\n",
      " Downloading shd_norm.mat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shd_norm.mat: 100%|██████████| 117M/117M [00:23<00:00, 5.31MB/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Downloaded: shd_norm.mat\n",
      " Downloading ssc_norm.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading ssc_norm.h5: 100%|██████████| 33.4M/33.4M [00:07<00:00, 4.89MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Downloaded: ssc_norm.h5\n",
      "\n",
      " All datasets downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def download_dataset(url, filename):\n",
    "    \"\"\"Download dataset from URL with progress bar.\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        print(f\" {filename} already exists, skipping download.\")\n",
    "        return\n",
    "    \n",
    "    print(f\" Downloading {filename}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024\n",
    "\n",
    "    with open(filename, \"wb\") as file, tqdm(\n",
    "        desc=f\"Downloading {filename}\",\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in response.iter_content(block_size):\n",
    "            file.write(data)\n",
    "            bar.update(len(data))\n",
    "    \n",
    "    print(f\" Downloaded: {filename}\")\n",
    "\n",
    "# Dataset URLs from Zenodo\n",
    "datasets = {\n",
    "    \"shd_norm.mat\": \"https://zenodo.org/record/16153275/files/shd_norm.mat\",\n",
    "    \"ssc_norm.h5\": \"https://zenodo.org/record/16153275/files/ssc_norm.h5\"\n",
    "}\n",
    "\n",
    "# Download both datasets\n",
    "print(\"=== Downloading Neuromorphic Datasets from Zenodo ===\")\n",
    "for filename, url in datasets.items():\n",
    "    download_dataset(url, filename)\n",
    "\n",
    "print(\"\\n All datasets downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d20a7",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Import necessary libraries for loading and preprocessing neuromorphic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c22098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy import io\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0598c1c3",
   "metadata": {},
   "source": [
    "## Dataset Loading and DataLoader Creation\n",
    "\n",
    "Load both SHD-Norm and SSC-Norm datasets and create PyTorch DataLoaders for easy batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29975592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Neuromorphic Datasets ===\n",
      " Loading SHD-Norm dataset...\n",
      "    Dataset shape: X=(5460, 224, 100), Y=(5460,)\n",
      "    Number of classes: 20\n",
      "    Class distribution: [273 273 273 273 273 273 273 273 273 273 273 273 273 273 273 273 273 273\n",
      " 273 273]\n",
      "     SHD-Norm DataLoaders created:\n",
      "      Train: 3276 samples (25 batches)\n",
      "      Val:   819 samples (7 batches)\n",
      "      Test:  819 samples (7 batches)\n",
      "\n",
      " Loading SSC-Norm dataset...\n",
      "     SHD-Norm DataLoaders created:\n",
      "      Train: 3276 samples (25 batches)\n",
      "      Val:   819 samples (7 batches)\n",
      "      Test:  819 samples (7 batches)\n",
      "\n",
      " Loading SSC-Norm dataset...\n",
      "    Dataset shape: X=(40390, 285, 100), Y=(40390,)\n",
      "    Number of classes: 35\n",
      "    Class distribution: [1154 1154 1154 1154 1154 1154 1154 1154 1154 1154 1154 1154 1154 1154\n",
      " 1154 1154 1154 1154 1154 1154 1154 1154 1154 1154 1154 1154 1154 1154\n",
      " 1154 1154 1154 1154 1154 1154 1154]\n",
      "    Dataset shape: X=(40390, 285, 100), Y=(40390,)\n",
      "    Number of classes: 35\n",
      "    Class distribution: [1154 1154 1154 1154 1154 1154 1154 1154 1154 1154 1154 1154 1154 1154\n",
      " 1154 1154 1154 1154 1154 1154 1154 1154 1154 1154 1154 1154 1154 1154\n",
      " 1154 1154 1154 1154 1154 1154 1154]\n",
      "     SSC-Norm DataLoaders created:\n",
      "      Train: 24234 samples (189 batches)\n",
      "      Val:   6058 samples (48 batches)\n",
      "      Test:  6059 samples (48 batches)\n",
      "\n",
      " All datasets loaded and DataLoaders created successfully!\n",
      "     SSC-Norm DataLoaders created:\n",
      "      Train: 24234 samples (189 batches)\n",
      "      Val:   6058 samples (48 batches)\n",
      "      Test:  6059 samples (48 batches)\n",
      "\n",
      " All datasets loaded and DataLoaders created successfully!\n"
     ]
    }
   ],
   "source": [
    "# === Dataset class for spike data ===\n",
    "class SpikeDataset(Dataset):\n",
    "    def __init__(self, X, Y, dataset_name=\"unknown\"):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.dataset_name = dataset_name\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.Y[idx]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# === Load SHD-Norm dataset (.mat format) ===\n",
    "def load_shd_norm():\n",
    "    \"\"\"Load SHD-Norm dataset from .mat file.\"\"\"\n",
    "    if not os.path.exists(\"shd_norm.mat\"):\n",
    "        print(\" shd_norm.mat not found. Please run the download cell first.\")\n",
    "        return None, None\n",
    "    \n",
    "    print(\" Loading SHD-Norm dataset...\")\n",
    "    data = io.loadmat(\"shd_norm.mat\")\n",
    "    X = data[\"X\"]  # shape: (samples, neurons, time_steps)\n",
    "    Y = data[\"Y\"].ravel()\n",
    "    \n",
    "    print(f\"    Dataset shape: X={X.shape}, Y={Y.shape}\")\n",
    "    print(f\"    Number of classes: {len(np.unique(Y))}\")\n",
    "    print(f\"    Class distribution: {np.bincount(Y)}\")\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "# === Load SSC-Norm dataset (.h5 format) ===\n",
    "def load_ssc_norm():\n",
    "    \"\"\"Load SSC-Norm dataset from .h5 file.\"\"\"\n",
    "    if not os.path.exists(\"ssc_norm.h5\"):\n",
    "        print(\" ssc_norm.h5 not found. Please run the download cell first.\")\n",
    "        return None, None\n",
    "    \n",
    "    print(\" Loading SSC-Norm dataset...\")\n",
    "    with h5py.File(\"ssc_norm.h5\", 'r') as f:\n",
    "        X = f[\"X\"][:].astype(np.float32)  # shape: (samples, neurons, time_steps)\n",
    "        Y = f[\"Y\"][:].ravel()\n",
    "    \n",
    "    print(f\"    Dataset shape: X={X.shape}, Y={Y.shape}\")\n",
    "    print(f\"    Number of classes: {len(np.unique(Y))}\")\n",
    "    print(f\"    Class distribution: {np.bincount(Y)}\")\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "# === Create DataLoaders ===\n",
    "def create_dataloaders(X, Y, dataset_name, batch_size=128, train_ratio=0.6, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Create train, validation, and test DataLoaders from dataset.\n",
    "    \n",
    "    Args:\n",
    "        X: Input spike data\n",
    "        Y: Labels\n",
    "        dataset_name: Name of the dataset for logging\n",
    "        batch_size: Batch size for DataLoaders\n",
    "        train_ratio: Proportion of data for training\n",
    "        val_ratio: Proportion of data for validation\n",
    "        test_ratio: Proportion of data for testing\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader\n",
    "    \"\"\"\n",
    "    if X is None or Y is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    N = len(Y)\n",
    "    \n",
    "    # Define splits\n",
    "    train_end = int(N * train_ratio)\n",
    "    val_end = int(N * (train_ratio + val_ratio))\n",
    "    test_end = int(N * (train_ratio + val_ratio + test_ratio))\n",
    "    \n",
    "    # Split indices\n",
    "    train_indices = np.arange(0, train_end)\n",
    "    val_indices = np.arange(train_end, val_end)\n",
    "    test_indices = np.arange(val_end, test_end)\n",
    "    \n",
    "    # Shuffle training indices\n",
    "    np.random.shuffle(train_indices)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SpikeDataset(X[train_indices], Y[train_indices], f\"{dataset_name}_train\")\n",
    "    val_dataset = SpikeDataset(X[val_indices], Y[val_indices], f\"{dataset_name}_val\")\n",
    "    test_dataset = SpikeDataset(X[test_indices], Y[test_indices], f\"{dataset_name}_test\")\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    \n",
    "    print(f\"     {dataset_name} DataLoaders created:\")\n",
    "    print(f\"      Train: {len(train_dataset)} samples ({len(train_loader)} batches)\")\n",
    "    print(f\"      Val:   {len(val_dataset)} samples ({len(val_loader)} batches)\")\n",
    "    print(f\"      Test:  {len(test_dataset)} samples ({len(test_loader)} batches)\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# === Load both datasets ===\n",
    "print(\"=== Loading Neuromorphic Datasets ===\")\n",
    "\n",
    "# Load SHD-Norm\n",
    "X_shd, Y_shd = load_shd_norm()\n",
    "shd_train_loader, shd_val_loader, shd_test_loader = create_dataloaders(\n",
    "    X_shd, Y_shd, \"SHD-Norm\", batch_size=128\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "# Load SSC-Norm  \n",
    "X_ssc, Y_ssc = load_ssc_norm()\n",
    "ssc_train_loader, ssc_val_loader, ssc_test_loader = create_dataloaders(\n",
    "    X_ssc, Y_ssc, \"SSC-Norm\", batch_size=128\n",
    ")\n",
    "\n",
    "print(\"\\n All datasets loaded and DataLoaders created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
